#!/usr/bin/env python3
"""
æµ‹è¯•è½¬æ¢åçš„éŸ³é¢‘æ–‡ä»¶ - éªŒè¯WAVæ–‡ä»¶å¹¶å±•ç¤ºä½¿ç”¨æ–¹æ³•
"""

import os
import numpy as np
import librosa
import soundfile as sf
import matplotlib.pyplot as plt

def test_audio_file(wav_path):
    """
    æµ‹è¯•éŸ³é¢‘æ–‡ä»¶çš„å„ç§åŠŸèƒ½
    
    Args:
        wav_path (str): WAVæ–‡ä»¶è·¯å¾„
    """
    print(f"ğŸµ æµ‹è¯•éŸ³é¢‘æ–‡ä»¶: {wav_path}")
    print("=" * 50)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(wav_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {wav_path}")
        return
    
    try:
        # 1. ä½¿ç”¨librosaåŠ è½½éŸ³é¢‘
        print("ğŸ“– ä½¿ç”¨librosaåŠ è½½éŸ³é¢‘...")
        audio_data, sample_rate = librosa.load(wav_path, sr=None)
        
        print(f"âœ… åŠ è½½æˆåŠŸ!")
        print(f"  ğŸ“Š é‡‡æ ·ç‡: {sample_rate} Hz")
        print(f"  ğŸ“ éŸ³é¢‘é•¿åº¦: {len(audio_data)} æ ·æœ¬")
        print(f"  â±ï¸  æ—¶é•¿: {len(audio_data) / sample_rate:.2f} ç§’")
        print(f"  ğŸ“ˆ æ•°æ®ç±»å‹: {audio_data.dtype}")
        print(f"  ğŸ“Š æ•°å€¼èŒƒå›´: [{audio_data.min():.4f}, {audio_data.max():.4f}]")
        
        # 2. éŸ³é¢‘ç‰¹å¾åˆ†æ
        print(f"\nğŸ” éŸ³é¢‘ç‰¹å¾åˆ†æ:")
        
        # RMSèƒ½é‡
        rms = librosa.feature.rms(y=audio_data)[0]
        print(f"  ğŸ”Š RMSèƒ½é‡: å¹³å‡={np.mean(rms):.4f}, æœ€å¤§={np.max(rms):.4f}")
        
        # é›¶äº¤å‰ç‡
        zcr = librosa.feature.zero_crossing_rate(audio_data)[0]
        print(f"  ğŸŒŠ é›¶äº¤å‰ç‡: å¹³å‡={np.mean(zcr):.4f}")
        
        # é¢‘è°±è´¨å¿ƒ
        spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate)[0]
        print(f"  ğŸ¼ é¢‘è°±è´¨å¿ƒ: å¹³å‡={np.mean(spectral_centroids):.2f} Hz")
        
        # 3. ä½¿ç”¨soundfileè¯»å–ï¼ˆéªŒè¯æ ¼å¼ï¼‰
        print(f"\nğŸ“– ä½¿ç”¨soundfileéªŒè¯...")
        sf_data, sf_sr = sf.read(wav_path)
        print(f"âœ… SoundFileè¯»å–æˆåŠŸ!")
        print(f"  ğŸ“Š é‡‡æ ·ç‡: {sf_sr} Hz")
        print(f"  ğŸ“ å½¢çŠ¶: {sf_data.shape}")
        print(f"  ğŸ“ˆ æ•°æ®ç±»å‹: {sf_data.dtype}")
        
        # 4. ä¿å­˜éŸ³é¢‘ç‰‡æ®µç¤ºä¾‹
        print(f"\nğŸ’¾ ä¿å­˜éŸ³é¢‘ç‰‡æ®µç¤ºä¾‹...")
        
        # ä¿å­˜å‰1ç§’çš„éŸ³é¢‘
        if len(audio_data) > sample_rate:
            clip_data = audio_data[:sample_rate]  # å‰1ç§’
            clip_path = "data/audio_clip_1sec.wav"
            sf.write(clip_path, clip_data, sample_rate)
            print(f"âœ… å·²ä¿å­˜1ç§’ç‰‡æ®µ: {clip_path}")
        
        # 5. ç®€å•çš„éŸ³é¢‘å¯è§†åŒ–
        print(f"\nğŸ“Š ç”ŸæˆéŸ³é¢‘å¯è§†åŒ–...")
        try:
            plt.figure(figsize=(12, 8))
            
            # æ—¶åŸŸæ³¢å½¢
            plt.subplot(3, 1, 1)
            time_axis = np.linspace(0, len(audio_data) / sample_rate, len(audio_data))
            plt.plot(time_axis, audio_data)
            plt.title('éŸ³é¢‘æ³¢å½¢ (æ—¶åŸŸ)')
            plt.xlabel('æ—¶é—´ (ç§’)')
            plt.ylabel('æŒ¯å¹…')
            plt.grid(True)
            
            # é¢‘è°±å›¾
            plt.subplot(3, 1, 2)
            D = librosa.amplitude_to_db(np.abs(librosa.stft(audio_data)), ref=np.max)
            librosa.display.specshow(D, y_axis='hz', x_axis='time', sr=sample_rate)
            plt.title('é¢‘è°±å›¾')
            plt.colorbar(format='%+2.0f dB')
            
            # RMSèƒ½é‡
            plt.subplot(3, 1, 3)
            times = librosa.frames_to_time(np.arange(len(rms)), sr=sample_rate)
            plt.plot(times, rms)
            plt.title('RMSèƒ½é‡')
            plt.xlabel('æ—¶é—´ (ç§’)')
            plt.ylabel('RMS')
            plt.grid(True)
            
            plt.tight_layout()
            plot_path = "data/audio_analysis.png"
            plt.savefig(plot_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {plot_path}")
            
        except Exception as e:
            print(f"âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        
        # 6. éŸ³é¢‘è´¨é‡æ£€æŸ¥
        print(f"\nğŸ” éŸ³é¢‘è´¨é‡æ£€æŸ¥:")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é™éŸ³
        silence_threshold = 0.01
        silent_samples = np.sum(np.abs(audio_data) < silence_threshold)
        silence_ratio = silent_samples / len(audio_data)
        print(f"  ğŸ”‡ é™éŸ³æ¯”ä¾‹: {silence_ratio:.2%}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å‰Šæ³¢
        clipping_threshold = 0.95
        clipped_samples = np.sum(np.abs(audio_data) > clipping_threshold)
        clipping_ratio = clipped_samples / len(audio_data)
        print(f"  âœ‚ï¸  å‰Šæ³¢æ¯”ä¾‹: {clipping_ratio:.2%}")
        
        # åŠ¨æ€èŒƒå›´
        dynamic_range = 20 * np.log10(np.max(np.abs(audio_data)) / (np.mean(np.abs(audio_data)) + 1e-10))
        print(f"  ğŸ“ åŠ¨æ€èŒƒå›´: {dynamic_range:.2f} dB")
        
        # 7. ä½¿ç”¨å»ºè®®
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print(f"  - éŸ³é¢‘è´¨é‡: {'è‰¯å¥½' if clipping_ratio < 0.01 and silence_ratio < 0.5 else 'éœ€è¦æ£€æŸ¥'}")
        print(f"  - é€‚åˆè¯­éŸ³è¯†åˆ«: {'æ˜¯' if np.mean(rms) > 0.01 and silence_ratio < 0.7 else 'å¯èƒ½éœ€è¦é¢„å¤„ç†'}")
        print(f"  - å¯ä»¥ç›´æ¥ç”¨äºè®­ç»ƒ: {'æ˜¯' if dynamic_range > 10 else 'å»ºè®®è¿›è¡ŒéŸ³é¢‘å¢å¼º'}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
        return False

def demonstrate_usage():
    """
    æ¼”ç¤ºå¦‚ä½•åœ¨å…¶ä»–ä»£ç ä¸­ä½¿ç”¨è½¬æ¢åçš„éŸ³é¢‘
    """
    print(f"\nğŸš€ ä»£ç ä½¿ç”¨ç¤ºä¾‹:")
    print("=" * 50)
    
    code_examples = [
        ("ä½¿ç”¨librosaåŠ è½½", """
import librosa
audio, sr = librosa.load('data/converted_audio.wav')
print(f"éŸ³é¢‘é•¿åº¦: {len(audio)} æ ·æœ¬, é‡‡æ ·ç‡: {sr} Hz")
"""),
        ("ä½¿ç”¨soundfileåŠ è½½", """
import soundfile as sf
audio, sr = sf.read('data/converted_audio.wav')
print(f"éŸ³é¢‘å½¢çŠ¶: {audio.shape}, é‡‡æ ·ç‡: {sr} Hz")
"""),
        ("æå–MFCCç‰¹å¾", """
import librosa
audio, sr = librosa.load('data/converted_audio.wav')
mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
print(f"MFCCç‰¹å¾å½¢çŠ¶: {mfccs.shape}")
"""),
        ("è¯­éŸ³è¯†åˆ«", """
import speech_recognition as sr
r = sr.Recognizer()
with sr.AudioFile('data/converted_audio.wav') as source:
    audio = r.record(source)
    text = r.recognize_google(audio, language='zh-CN')
    print(f"è¯†åˆ«ç»“æœ: {text}")
"""),
        ("éŸ³é¢‘åˆ†æ®µ", """
import librosa
audio, sr = librosa.load('data/converted_audio.wav')
# æŒ‰1ç§’åˆ†æ®µ
segment_length = sr  # 1ç§’çš„æ ·æœ¬æ•°
segments = [audio[i:i+segment_length] for i in range(0, len(audio), segment_length)]
print(f"åˆ†æˆ {len(segments)} æ®µ")
""")
    ]
    
    for title, code in code_examples:
        print(f"\nğŸ“ {title}:")
        print(code.strip())

def main():
    """ä¸»å‡½æ•°"""
    wav_file = "data/converted_audio.wav"
    
    # æµ‹è¯•éŸ³é¢‘æ–‡ä»¶
    success = test_audio_file(wav_file)
    
    if success:
        # æ¼”ç¤ºä½¿ç”¨æ–¹æ³•
        demonstrate_usage()
        
        print(f"\nğŸ‰ éŸ³é¢‘æ–‡ä»¶æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“ ä¸»è¦è¾“å‡ºæ–‡ä»¶:")
        print(f"  - åŸå§‹éŸ³é¢‘: {wav_file}")
        print(f"  - éŸ³é¢‘ç‰‡æ®µ: data/audio_clip_1sec.wav")
        print(f"  - åˆ†æå›¾è¡¨: data/audio_analysis.png")
    else:
        print(f"\nâŒ éŸ³é¢‘æ–‡ä»¶æµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    main() 